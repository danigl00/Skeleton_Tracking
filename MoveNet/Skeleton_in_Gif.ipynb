{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.- Preparing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\p0121182\\AppData\\Local\\anaconda3\\envs\\SkeletonTracking\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import imageio \n",
    "import matplotlib.pyplot as plt \n",
    "from IPython.display import HTML, display\n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue = (26, 128, 187)\n",
    "orange = (234, 128, 28)\n",
    "\n",
    "EDGE_COLORS = {\n",
    "    (0, 1): blue,\n",
    "    (0, 2): orange,\n",
    "    (1, 3): blue,\n",
    "    (2, 4): orange,\n",
    "    (0, 5): blue,\n",
    "    (0, 6): orange,\n",
    "    (5, 7): blue,\n",
    "    (7, 9): blue,\n",
    "    (6, 8): orange,\n",
    "    (8, 10): orange,\n",
    "    (5, 6): blue,\n",
    "    (5, 11): blue,\n",
    "    (6, 12): orange,\n",
    "    (11, 12): orange,\n",
    "    (11, 13): blue,\n",
    "    (13, 15): blue,\n",
    "    (12, 14): orange,\n",
    "    (14, 16): orange\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model from Tensor Flow Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lightning : intended for latency-critical applications\n",
    "\\\n",
    "Thunder : for applications that require high accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path='./Models/movenet_thunder_f16.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "def movenet(input_image):\n",
    "    \"\"\"Runs detection on an input image.\n",
    "\n",
    "    Args:\n",
    "      input_image: A [1, height, width, 3] tensor represents the input image\n",
    "        pixels. Note that the height/width should already be resized and match the\n",
    "        expected input resolution of the model before passing into this function.\n",
    "\n",
    "    Returns:\n",
    "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "      coordinates and scores.\n",
    "    \"\"\"\n",
    "    # TF Lite format expects tensor type of uint8.\n",
    "    input_image = tf.cast(input_image, dtype=tf.uint8)\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n",
    "    # Invoke inference.\n",
    "    interpreter.invoke()\n",
    "    # Get the model prediction.\n",
    "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return keypoints_with_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = hub.load(\"https://www.kaggle.com/models/google/movenet/TensorFlow2/multipose-lightning/1\")\n",
    "#movenet = model.signatures['serving_default']\n",
    "\n",
    "\n",
    "def movenet(input_image):\n",
    "    \"\"\"Runs detection on an input image.\n",
    "\n",
    "    Args:\n",
    "      input_image: A [1, height, width, 3] tensor represents the input image\n",
    "        pixels. Note that the height/width should already be resized and match the\n",
    "        expected input resolution of the model before passing into this function.\n",
    "\n",
    "    Returns:\n",
    "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "      coordinates and scores.\n",
    "    \"\"\"\n",
    "    model = module.signatures['serving_default']\n",
    "\n",
    "    # SavedModel format expects tensor type of int32.\n",
    "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "    # Run model inference.\n",
    "    outputs = model(input_image)\n",
    "    # Output is a [1, 1, 17, 3] tensor.\n",
    "    keypoints_with_scores = outputs['output_0'].numpy()\n",
    "    return keypoints_with_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial_width, initial_height = (461,250)\n",
    "WIDTH = HEIGHT = 192"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.- Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(frame, keypoints, threshold=0.11):\n",
    "    \"\"\"\n",
    "    Main loop : Draws the keypoints and edges for each instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Loop through the results\n",
    "    for instance in keypoints: \n",
    "        # Draw the keypoints\n",
    "        denormalized_coordinates = draw_keypoints(frame, instance, threshold)\n",
    "        # Draw the edges\n",
    "        draw_edges(denormalized_coordinates, frame, EDGE_COLORS, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoints(frame, keypoints, threshold=0.11):\n",
    "    \"\"\"Draws the keypoints on a image frame\"\"\"\n",
    "    \n",
    "    # Denormalize the coordinates of the keypoints \n",
    "    denormalized_coordinates = np.squeeze(np.multiply(keypoints, [WIDTH,HEIGHT,1]))\n",
    "    for keypoint in denormalized_coordinates:\n",
    "        # Unpack the keypoint values\n",
    "        keypoint_y, keypoint_x, keypoint_confidence = keypoint\n",
    "        if keypoint_confidence > threshold:\n",
    "            # Draw the keypoints\n",
    "            cv2.circle(\n",
    "                img=frame, \n",
    "                center=(int(keypoint_x), int(keypoint_y)), \n",
    "                radius=4, \n",
    "                color=(255,0,0),\n",
    "                thickness=-1\n",
    "            )\n",
    "    return denormalized_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_edges(denormalized_coordinates, frame, edges_colors, threshold=0.11):\n",
    "    for edge, color in edges_colors.items():\n",
    "        # Get the dict value associated to the actual edge\n",
    "        p1, p2 = edge\n",
    "        # Get the points\n",
    "        y1, x1, confidence_1 = denormalized_coordinates[p1]\n",
    "        y2, x2, confidence_2 = denormalized_coordinates[p2]\n",
    "        # Draw the line from point 1 to point 2, the confidence > threshold\n",
    "        if (confidence_1 > threshold) & (confidence_2 > threshold):      \n",
    "            cv2.line(\n",
    "                img=frame, \n",
    "                pt1=(int(x1), int(y1)),\n",
    "                pt2=(int(x2), int(y2)), \n",
    "                color=color, \n",
    "                thickness=2, \n",
    "                lineType=cv2.LINE_AA\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def progress(value, max=100):\n",
    "    return HTML(\"\"\"\n",
    "      <progress\n",
    "          value='{value}'\n",
    "          max='{max}',\n",
    "          style='width: 100%'\n",
    "      >\n",
    "          {value}\n",
    "      </progress>\n",
    "  \"\"\".format(value=value,\n",
    "                max=max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process each frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gif():    \n",
    "    # Load the gif\n",
    "    gif = cv2.VideoCapture(\"../Sample_gifs/Spiderman3.gif\")\n",
    "    # Get the frame count\n",
    "    frame_count = int(gif.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    # Display parameter\n",
    "    print(f\"Frame count: {frame_count}\")\n",
    "    \n",
    "    \"\"\"\"\"\n",
    "    Initialize the video writer \n",
    "    We'll append each frame and its drawing to a vector, then stack all the frames to obtain a sequence (video). \n",
    "    \"\"\"\n",
    "    output_frames = []\n",
    "    \n",
    "    # Get the initial shape (width, height)\n",
    "    initial_shape = []\n",
    "    initial_shape.append(int(gif.get(cv2.CAP_PROP_FRAME_WIDTH)))\n",
    "    initial_shape.append(int(gif.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    \n",
    "    return gif, frame_count, output_frames, initial_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference():\n",
    "    \"\"\"\n",
    "    Runs inferences then starts the main loop for each frame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the gif\n",
    "    gif, frame_count, output_frames, initial_shape = load_gif()\n",
    "    # Set the progress bar to 0. It ranges from the first to the last frame\n",
    "    bar = display(progress(0, frame_count-1), display_id=True)\n",
    "    \n",
    "    # Loop while the gif is opened\n",
    "    while gif.isOpened():\n",
    "        \n",
    "        # Capture the frame\n",
    "        ret, frame = gif.read()\n",
    "        \n",
    "        # Exit if the frame is empty\n",
    "        if frame is None: \n",
    "            break\n",
    "        \n",
    "        # Retrieve the frame index\n",
    "        current_index = gif.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "        \n",
    "        # Copy the frame\n",
    "        image = frame.copy()\n",
    "        image = cv2.resize(image, (WIDTH,HEIGHT))\n",
    "        # Resize to the target shape and cast to an int32 vector\n",
    "        input_image = tf.cast(tf.image.resize_with_pad(image, WIDTH, HEIGHT), dtype=tf.int32)\n",
    "        # Create a batch (input tensor)\n",
    "        input_image = tf.expand_dims(input_image, axis=0)\n",
    "\n",
    "        # Perform inference\n",
    "        results = movenet(input_image)\n",
    "        \"\"\"\n",
    "        Output shape :  [1, 6, 56] ---> (batch size), (instances), (xy keypoints coordinates and score from [0:50] \n",
    "        and [ymin, xmin, ymax, xmax, score] for the remaining elements)\n",
    "        First, let's resize it to a more convenient shape, following this logic : \n",
    "        - First channel ---> each instance\n",
    "        - Second channel ---> 17 keypoints for each instance\n",
    "        - The 51st values of the last channel ----> the confidence score.\n",
    "        Thus, the Tensor is reshaped without losing important information. \n",
    "        \"\"\"\n",
    "\n",
    "        # Loop through the results\n",
    "        loop(image, results, threshold=0.11)\n",
    "        \n",
    "        # Get the output frame : reshape to the original size\n",
    "        frame_rgb = cv2.cvtColor(\n",
    "            cv2.resize(\n",
    "                image,(initial_shape[0], initial_shape[1]), \n",
    "                interpolation=cv2.INTER_LANCZOS4\n",
    "            ), \n",
    "            cv2.COLOR_BGR2RGB # OpenCV processes BGR images instead of RGB\n",
    "        ) \n",
    "        \n",
    "        # Add the drawings to the output frames\n",
    "        output_frames.append(frame_rgb)\n",
    "        \n",
    "        # Update the progress bar\n",
    "        bar.update(progress(current_index, frame_count-1))\n",
    "    \n",
    "    # Release the object\n",
    "    gif.release()\n",
    "    \n",
    "    print(\"Complete\")\n",
    "    \n",
    "    return output_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame count: 58\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <progress\n",
       "          value='0'\n",
       "          max='57',\n",
       "          style='width: 100%'\n",
       "      >\n",
       "          0\n",
       "      </progress>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (6,56) (3,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m output_frames \u001b[38;5;241m=\u001b[39m \u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[79], line 45\u001b[0m, in \u001b[0;36mrun_inference\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03mOutput shape :  [1, 6, 56] ---> (batch size), (instances), (xy keypoints coordinates and score from [0:50] \u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03mand [ymin, xmin, ymax, xmax, score] for the remaining elements)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03mThus, the Tensor is reshaped without losing important information. \u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Loop through the results\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m \u001b[43mloop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.11\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Get the output frame : reshape to the original size\u001b[39;00m\n\u001b[0;32m     48\u001b[0m frame_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(\n\u001b[0;32m     49\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mresize(\n\u001b[0;32m     50\u001b[0m         image,(initial_shape[\u001b[38;5;241m0\u001b[39m], initial_shape[\u001b[38;5;241m1\u001b[39m]), \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     53\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB \u001b[38;5;66;03m# OpenCV processes BGR images instead of RGB\u001b[39;00m\n\u001b[0;32m     54\u001b[0m ) \n",
      "Cell \u001b[1;32mIn[76], line 9\u001b[0m, in \u001b[0;36mloop\u001b[1;34m(frame, keypoints, threshold)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Loop through the results\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m instance \u001b[38;5;129;01min\u001b[39;00m keypoints: \n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Draw the keypoints\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     denormalized_coordinates \u001b[38;5;241m=\u001b[39m \u001b[43mdraw_keypoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Draw the edges\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     draw_edges(denormalized_coordinates, frame, EDGE_COLORS, threshold)\n",
      "Cell \u001b[1;32mIn[77], line 5\u001b[0m, in \u001b[0;36mdraw_keypoints\u001b[1;34m(frame, keypoints, threshold)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Draws the keypoints on a image frame\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Denormalize the coordinates of the keypoints \u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m denormalized_coordinates \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeypoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mWIDTH\u001b[49m\u001b[43m,\u001b[49m\u001b[43mHEIGHT\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keypoint \u001b[38;5;129;01min\u001b[39;00m denormalized_coordinates:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Unpack the keypoint values\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     keypoint_y, keypoint_x, keypoint_confidence \u001b[38;5;241m=\u001b[39m keypoint\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (6,56) (3,) "
     ]
    }
   ],
   "source": [
    "output_frames = run_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_docs.vis import embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 220, 3), |u1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\p0121182\\AppData\\Local\\anaconda3\\envs\\SkeletonTracking\\lib\\site-packages\\PIL\\Image.py:3098\u001b[0m, in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   3097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3098\u001b[0m     mode, rawmode \u001b[38;5;241m=\u001b[39m \u001b[43m_fromarray_typemap\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtypekey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   3099\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyError\u001b[0m: ((1, 1, 220, 3), '|u1')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(output_frames, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Write the sequence to a gif\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mimageio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmimsave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./test.gif\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Embed the output to the notebook\u001b[39;00m\n\u001b[0;32m      6\u001b[0m embed\u001b[38;5;241m.\u001b[39membed_file(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./test.gif\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n",
      "File \u001b[1;32mc:\\Users\\p0121182\\AppData\\Local\\anaconda3\\envs\\SkeletonTracking\\lib\\site-packages\\imageio\\v2.py:495\u001b[0m, in \u001b[0;36mmimwrite\u001b[1;34m(uri, ims, format, **kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m imopen_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegacy_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m imopen(uri, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mimopen_args) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m--> 495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mwrite(ims, is_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\p0121182\\AppData\\Local\\anaconda3\\envs\\SkeletonTracking\\lib\\site-packages\\imageio\\plugins\\pillow.py:444\u001b[0m, in \u001b[0;36mPillowPlugin.write\u001b[1;34m(self, ndimage, mode, format, is_batch, **kwargs)\u001b[0m\n\u001b[0;32m    441\u001b[0m     ndimage \u001b[38;5;241m=\u001b[39m ndimage[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m ndimage:\n\u001b[1;32m--> 444\u001b[0m     pil_frame \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbits\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m    446\u001b[0m         pil_frame \u001b[38;5;241m=\u001b[39m pil_frame\u001b[38;5;241m.\u001b[39mquantize(colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbits\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\p0121182\\AppData\\Local\\anaconda3\\envs\\SkeletonTracking\\lib\\site-packages\\PIL\\Image.py:3102\u001b[0m, in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   3100\u001b[0m         typekey_shape, typestr \u001b[38;5;241m=\u001b[39m typekey\n\u001b[0;32m   3101\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot handle this data type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypekey_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypestr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 3102\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   3103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3104\u001b[0m     rawmode \u001b[38;5;241m=\u001b[39m mode\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 220, 3), |u1"
     ]
    }
   ],
   "source": [
    "# Stack the output frames horizontally to compose a sequence\n",
    "output = np.stack(output_frames, axis=0) \n",
    "# Write the sequence to a gif\n",
    "imageio.mimsave(\"./test.gif\", output, fps=20) \n",
    "# Embed the output to the notebook\n",
    "embed.embed_file(\"./test.gif\") \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SkeletonTracking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
