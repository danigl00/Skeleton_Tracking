{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.- Preparing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\p0121182\\AppData\\Local\\anaconda3\\envs\\SkeletonTracking\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import imageio \n",
    "import matplotlib.pyplot as plt \n",
    "from IPython.display import HTML, display\n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue = (26, 128, 187)\n",
    "orange = (234, 128, 28)\n",
    "\n",
    "EDGE_COLORS = {\n",
    "    (0, 1): blue,\n",
    "    (0, 2): orange,\n",
    "    (1, 3): blue,\n",
    "    (2, 4): orange,\n",
    "    (0, 5): blue,\n",
    "    (0, 6): orange,\n",
    "    (5, 7): blue,\n",
    "    (7, 9): blue,\n",
    "    (6, 8): orange,\n",
    "    (8, 10): orange,\n",
    "    (5, 6): blue,\n",
    "    (5, 11): blue,\n",
    "    (6, 12): orange,\n",
    "    (11, 12): orange,\n",
    "    (11, 13): blue,\n",
    "    (13, 15): blue,\n",
    "    (12, 14): orange,\n",
    "    (14, 16): orange\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model from Tensor Flow Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lightning : intended for latency-critical applications\n",
    "\\\n",
    "Thunder : for applications that require high accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Trying to load a model of incompatible/unknown type. 'C:\\Users\\p0121182\\AppData\\Local\\Temp\\tfhub_modules\\312f001449331ee3d410d758fccdc9945a65dbc3' contains neither 'saved_model.pb' nor 'saved_model.pbtxt'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://tfhub.dev/google/movenet/multipose/lightning/1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m movenet \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39msignatures[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserving_default\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\p0121182\\AppData\\Local\\anaconda3\\envs\\SkeletonTracking\\lib\\site-packages\\tensorflow_hub\\module_v2.py:113\u001b[0m, in \u001b[0;36mload\u001b[1;34m(handle, tags, options)\u001b[0m\n\u001b[0;32m    108\u001b[0m saved_model_pbtxt_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m    109\u001b[0m     tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mas_bytes(module_path),\n\u001b[0;32m    110\u001b[0m     tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mas_bytes(tf\u001b[38;5;241m.\u001b[39msaved_model\u001b[38;5;241m.\u001b[39mSAVED_MODEL_FILENAME_PBTXT))\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(saved_model_path) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(saved_model_pbtxt_path)):\n\u001b[1;32m--> 113\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to load a model of incompatible/unknown type. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    114\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m contains neither \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m nor \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m    115\u001b[0m                    (module_path, tf\u001b[38;5;241m.\u001b[39msaved_model\u001b[38;5;241m.\u001b[39mSAVED_MODEL_FILENAME_PB,\n\u001b[0;32m    116\u001b[0m                     tf\u001b[38;5;241m.\u001b[39msaved_model\u001b[38;5;241m.\u001b[39mSAVED_MODEL_FILENAME_PBTXT))\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m options:\n\u001b[0;32m    119\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(tf, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoadOptions\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: Trying to load a model of incompatible/unknown type. 'C:\\Users\\p0121182\\AppData\\Local\\Temp\\tfhub_modules\\312f001449331ee3d410d758fccdc9945a65dbc3' contains neither 'saved_model.pb' nor 'saved_model.pbtxt'."
     ]
    }
   ],
   "source": [
    "model = hub.load(\"https://tfhub.dev/google/movenet/multipose/lightning/1\")\n",
    "movenet = model.signatures[\"serving_default\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial_width, initial_height = (461,250)\n",
    "WIDTH = HEIGHT = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.- Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(frame, keypoints, threshold=0.11):\n",
    "    \"\"\"\n",
    "    Main loop : Draws the keypoints and edges for each instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Loop through the results\n",
    "    for instance in keypoints: \n",
    "        # Draw the keypoints\n",
    "        denormalized_coordinates = draw_keypoints(frame, instance, threshold)\n",
    "        # Draw the edges\n",
    "        draw_edges(denormalized_coordinates, frame, EDGE_COLORS, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoints(frame, keypoints, threshold=0.11):\n",
    "    \"\"\"Draws the keypoints on a image frame\"\"\"\n",
    "    \n",
    "    # Denormalize the coordinates of the keypoints \n",
    "    denormalized_coordinates = np.squeeze(np.multiply(keypoints, [WIDTH,HEIGHT,1]))\n",
    "    for keypoint in denormalized_coordinates:\n",
    "        # Unpack the keypoint values\n",
    "        keypoint_y, keypoint_x, keypoint_confidence = keypoint\n",
    "        if keypoint_confidence > threshold:\n",
    "            # Draw the keypoints\n",
    "            cv2.circle(\n",
    "                img=frame, \n",
    "                center=(int(keypoint_x), int(keypoint_y)), \n",
    "                radius=4, \n",
    "                color=(255,0,0),\n",
    "                thickness=-1\n",
    "            )\n",
    "    return denormalized_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_edges(denormalized_coordinates, frame, edges_colors, threshold=0.11):\n",
    "    for edge, color in edges_colors.items():\n",
    "        # Get the dict value associated to the actual edge\n",
    "        p1, p2 = edge\n",
    "        # Get the points\n",
    "        y1, x1, confidence_1 = denormalized_coordinates[p1]\n",
    "        y2, x2, confidence_2 = denormalized_coordinates[p2]\n",
    "        # Draw the line from point 1 to point 2, the confidence > threshold\n",
    "        if (confidence_1 > threshold) & (confidence_2 > threshold):      \n",
    "            cv2.line(\n",
    "                img=frame, \n",
    "                pt1=(int(x1), int(y1)),\n",
    "                pt2=(int(x2), int(y2)), \n",
    "                color=color, \n",
    "                thickness=2, \n",
    "                lineType=cv2.LINE_AA\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def progress(value, max=100):\n",
    "    return HTML(\"\"\"\n",
    "      <progress\n",
    "          value='{value}'\n",
    "          max='{max}',\n",
    "          style='width: 100%'\n",
    "      >\n",
    "          {value}\n",
    "      </progress>\n",
    "  \"\"\".format(value=value,\n",
    "                max=max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process each frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gif():    \n",
    "    # Load the gif\n",
    "    gif = cv2.VideoCapture(\"./EMU/Output4.gif\")\n",
    "    # Get the frame count\n",
    "    frame_count = int(gif.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    # Display parameter\n",
    "    print(f\"Frame count: {frame_count}\")\n",
    "    \n",
    "    \"\"\"\"\"\n",
    "    Initialize the video writer \n",
    "    We'll append each frame and its drawing to a vector, then stack all the frames to obtain a sequence (video). \n",
    "    \"\"\"\n",
    "    output_frames = []\n",
    "    \n",
    "    # Get the initial shape (width, height)\n",
    "    initial_shape = []\n",
    "    initial_shape.append(int(gif.get(cv2.CAP_PROP_FRAME_WIDTH)))\n",
    "    initial_shape.append(int(gif.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    \n",
    "    return gif, frame_count, output_frames, initial_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference():\n",
    "    \"\"\"\n",
    "    Runs inferences then starts the main loop for each frame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the gif\n",
    "    gif, frame_count, output_frames, initial_shape = load_gif()\n",
    "    # Set the progress bar to 0. It ranges from the first to the last frame\n",
    "    bar = display(progress(0, frame_count-1), display_id=True)\n",
    "    \n",
    "    # Loop while the gif is opened\n",
    "    while gif.isOpened():\n",
    "        \n",
    "        # Capture the frame\n",
    "        ret, frame = gif.read()\n",
    "        \n",
    "        # Exit if the frame is empty\n",
    "        if frame is None: \n",
    "            break\n",
    "        \n",
    "        # Retrieve the frame index\n",
    "        current_index = gif.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "        \n",
    "        # Copy the frame\n",
    "        image = frame.copy()\n",
    "        image = cv2.resize(image, (WIDTH,HEIGHT))\n",
    "        # Resize to the target shape and cast to an int32 vector\n",
    "        input_image = tf.cast(tf.image.resize_with_pad(image, WIDTH, HEIGHT), dtype=tf.int32)\n",
    "        # Create a batch (input tensor)\n",
    "        input_image = tf.expand_dims(input_image, axis=0)\n",
    "\n",
    "        # Perform inference\n",
    "        results = movenet(input_image)\n",
    "        \"\"\"\n",
    "        Output shape :  [1, 6, 56] ---> (batch size), (instances), (xy keypoints coordinates and score from [0:50] \n",
    "        and [ymin, xmin, ymax, xmax, score] for the remaining elements)\n",
    "        First, let's resize it to a more convenient shape, following this logic : \n",
    "        - First channel ---> each instance\n",
    "        - Second channel ---> 17 keypoints for each instance\n",
    "        - The 51st values of the last channel ----> the confidence score.\n",
    "        Thus, the Tensor is reshaped without losing important information. \n",
    "        \"\"\"\n",
    "        \n",
    "        keypoints = results[\"output_0\"].numpy()[:,:,:51].reshape((6,17,3))\n",
    "\n",
    "        # Loop through the results\n",
    "        loop(image, keypoints, threshold=0.11)\n",
    "        \n",
    "        # Get the output frame : reshape to the original size\n",
    "        frame_rgb = cv2.cvtColor(\n",
    "            cv2.resize(\n",
    "                image,(initial_shape[0], initial_shape[1]), \n",
    "                interpolation=cv2.INTER_LANCZOS4\n",
    "            ), \n",
    "            cv2.COLOR_BGR2RGB # OpenCV processes BGR images instead of RGB\n",
    "        ) \n",
    "        \n",
    "        # Add the drawings to the output frames\n",
    "        output_frames.append(frame_rgb)\n",
    "        \n",
    "        # Update the progress bar\n",
    "        bar.update(progress(current_index, frame_count-1))\n",
    "    \n",
    "    # Release the object\n",
    "    gif.release()\n",
    "    \n",
    "    print(\"Complete\")\n",
    "    \n",
    "    return output_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame count: 1814\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <progress\n",
       "          value='1814.0'\n",
       "          max='1813',\n",
       "          style='width: 100%'\n",
       "      >\n",
       "          1814.0\n",
       "      </progress>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "output_frames = run_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_docs.vis import embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Gif\\\\EMU\\\\EMU4.gif'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m imageio\u001b[38;5;241m.\u001b[39mmimsave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./EMU/EMU4.gif\u001b[39m\u001b[38;5;124m\"\u001b[39m, output, fps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m) \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Embed the output to the notebook\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43membed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./Gif/EMU/EMU4.gif\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "File \u001b[1;32mc:\\Users\\p0121182\\AppData\\Local\\anaconda3\\envs\\SkeletonTracking\\lib\\site-packages\\tensorflow_docs\\vis\\embed.py:47\u001b[0m, in \u001b[0;36membed_file\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     45\u001b[0m path \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(path)\n\u001b[0;32m     46\u001b[0m mime, unused_encoding \u001b[38;5;241m=\u001b[39m mimetypes\u001b[38;5;241m.\u001b[39mguess_type(\u001b[38;5;28mstr\u001b[39m(path))\n\u001b[1;32m---> 47\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embed_data(mime, data)\n",
      "File \u001b[1;32mc:\\Users\\p0121182\\AppData\\Local\\anaconda3\\envs\\SkeletonTracking\\lib\\pathlib.py:1259\u001b[0m, in \u001b[0;36mPath.read_bytes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1257\u001b[0m \u001b[38;5;124;03m    Open the file in bytes mode, read it, and close the file.\u001b[39;00m\n\u001b[0;32m   1258\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1259\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m   1260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\p0121182\\AppData\\Local\\anaconda3\\envs\\SkeletonTracking\\lib\\pathlib.py:1252\u001b[0m, in \u001b[0;36mPath.open\u001b[1;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, buffering\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1247\u001b[0m          errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1248\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;124;03m    Open the file pointed by this path and return a file object, as\u001b[39;00m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;124;03m    the built-in open() function does.\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mopener\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\p0121182\\AppData\\Local\\anaconda3\\envs\\SkeletonTracking\\lib\\pathlib.py:1120\u001b[0m, in \u001b[0;36mPath._opener\u001b[1;34m(self, name, flags, mode)\u001b[0m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_opener\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, flags, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0o666\u001b[39m):\n\u001b[0;32m   1119\u001b[0m     \u001b[38;5;66;03m# A stub for the opener argument to built-in open()\u001b[39;00m\n\u001b[1;32m-> 1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Gif\\\\EMU\\\\EMU4.gif'"
     ]
    }
   ],
   "source": [
    "# Stack the output frames horizontally to compose a sequence\n",
    "output = np.stack(output_frames, axis=0) \n",
    "# Write the sequence to a gif\n",
    "imageio.mimsave(\"./EMU/EMU4.gif\", output, fps=20) \n",
    "# Embed the output to the notebook\n",
    "embed.embed_file(\"./EMU/EMU4.gif\") \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SkeletonTracking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
